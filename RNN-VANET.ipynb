{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT : A Hybrid Optimized VANET Routing Protocol Based on RNN and LPC for Reliable Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 🌐 Standard Library\n",
    "# =======================\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# =======================\n",
    "# 📦 Third-Party Libraries\n",
    "# =======================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "\n",
    "# =======================\n",
    "# 🧠 Scikit-learn\n",
    "# =======================\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =======================\n",
    "# 🔥 PyTorch\n",
    "# =======================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "# =======================\n",
    "# 📊 Visualization (Jupyter)\n",
    "# =======================\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "\n",
    "# =======================\n",
    "# 🎨 Miscellaneous\n",
    "# =======================\n",
    "from colorama import Back, Fore, Style\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold # multi-label classification task\n",
    "from sklearn.model_selection import StratifiedKFold  # single-label classification task\n",
    "from numba import cuda\n",
    "\n",
    "# =======================\n",
    "# 🛠️ Debug Mode\n",
    "# =======================\n",
    "debug = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check and use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a CUDA GPU is available\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2anVFzBXGdwH"
   },
   "source": [
    "# Import and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ohXIxzt4_U2"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path = \"../dataset.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Convert to a single 2D tensor\n",
    "data = torch.tensor([[float(x) for x in line.strip().split(\",\")] for line in lines])\n",
    "\n",
    "\n",
    "data = data.view(data.shape[0], 10)\n",
    "\n",
    "print(data.shape)  # Check dimensions\n",
    "# print(data)  # Print the tensor\n",
    "\n",
    "print(data.shape)\n",
    "# Define split sizes\n",
    "train_dev_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_dev_size\n",
    "\n",
    "# Perform the split\n",
    "train_dev_data, test_data = random_split(data, [train_dev_size, test_size])\n",
    "\n",
    "# Convert to tensors\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "train_dev_data = torch.stack([data[i] for i in train_dev_data.indices])\n",
    "test_data = torch.stack([data[i] for i in test_data.indices])\n",
    "\n",
    "print(train_dev_data.shape)  # Should match (train_size, 10)\n",
    "print(test_data.shape)   # Should match (dev_size, 10)\n",
    "\n",
    "print(f\"Train Dev Data Size: {len(train_dev_data)}\")\n",
    "print(f\"Test Data Size: {len(test_data)}\")\n",
    "\n",
    "\n",
    "# Pad sequences to the same length\n",
    "# data = pad_sequence(data, batch_first=True, padding_value=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to check if all labels exist in test data\n",
    "print(\"Train Dev dataset : \")\n",
    "labels_train_dev, counts_train_dev = torch.unique(train_dev_data[:, -1], return_counts=True)\n",
    "train_dev_count_gt = 0\n",
    "train_dev_count_le = 0\n",
    "\n",
    "for label, count in zip(labels_train_dev, counts_train_dev):\n",
    "    # print(f\"Label {label.item()} → {count.item()} samples\")\n",
    "    if label.item() > 103:\n",
    "        train_dev_count_gt += count.item()\n",
    "    else:\n",
    "        train_dev_count_le += count.item()\n",
    "\n",
    "print(f\"Count of numbers > 103 (Link Broken): {train_dev_count_gt}\")\n",
    "print(f\"Count of numbers <= 103 (Link Stable): {train_dev_count_le}\")\n",
    "\n",
    "print(\"Test dataset : \")\n",
    "labels_test, counts_test = torch.unique(test_data[:, -1], return_counts=True)\n",
    "test_count_gt = 0\n",
    "test_count_le = 0\n",
    "\n",
    "for label, count in zip(labels_test, counts_test):\n",
    "    # print(f\"Label {label.item()} → {count.item()} samples\")\n",
    "    if label.item() > 103:\n",
    "        test_count_gt += count.item()\n",
    "    else:\n",
    "        test_count_le += count.item()\n",
    "\n",
    "print(f\"Count of numbers > 103 (link broken): {test_count_gt}\")\n",
    "print(f\"Count of numbers <= 103 (link stable): {test_count_le}\")\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# First pair (train_dev)\n",
    "axs[0].bar(['link broken', 'link stable'], [train_dev_count_gt, train_dev_count_le], color=['blue', 'green'])\n",
    "axs[0].set_title('Train Dev Counts')\n",
    "axs[0].set_ylabel('Count')\n",
    "\n",
    "# Second pair (test)\n",
    "axs[1].bar(['link broken', 'link stable'], [test_count_gt, test_count_le], color=['orange', 'purple'])\n",
    "axs[1].set_title('Test Counts')\n",
    "axs[1].set_ylabel('Count')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VToReEHNWP0n"
   },
   "source": [
    "# Create a class for the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVcrKOC1Wqk-"
   },
   "outputs": [],
   "source": [
    "def createModel(optimizer_name, learningRate, input_size, num_layers, hidden_size):\n",
    "\n",
    "    class RNNNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dropout_prob = 0.3\n",
    "\n",
    "            # RNN layer\n",
    "            self.rnn = nn.RNN(\n",
    "                input_size=input_size,   # Size of input features per time step\n",
    "                hidden_size=hidden_size,  # Size of the hidden state\n",
    "                num_layers=num_layers, # Number of RNN layers\n",
    "                nonlinearity='tanh',  # or 'relu'\n",
    "                bias=True,    # Include bias weights or not\n",
    "                batch_first=True,  # True if input/output tensors are (batch, seq, feature)\n",
    "                dropout=self.dropout_prob if num_layers > 1 else 0,  # Dropout between layers (if num_layers > 1)\n",
    "                bidirectional=False  # True for bidirectional RNN not my case False\n",
    "            )\n",
    "                        \n",
    "            self.bn = nn.BatchNorm1d(hidden_size)\n",
    "            self.dropout = nn.Dropout(self.dropout_prob)\n",
    "            self.fc = nn.Linear(hidden_size, 1)  # Regression output\n",
    "\n",
    "        def forward(self, x, BN=False, dropout=False):\n",
    "            rnn_out, _ = self.rnn(x)\n",
    "            out = rnn_out[:, -1, :]  # Last time step\n",
    "\n",
    "            if BN:\n",
    "                out = self.bn(out)\n",
    "\n",
    "            out = F.relu(out)\n",
    "\n",
    "            if dropout:\n",
    "                out = self.dropout(out)\n",
    "\n",
    "            out = self.fc(out)  # No activation for regression\n",
    "            return out\n",
    "\n",
    "    # Create model\n",
    "    model = RNNNet()\n",
    "\n",
    "    # Regression loss function\n",
    "    lossfun = nn.MSELoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=learningRate)\n",
    "\n",
    "    return model, lossfun, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if there is no error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WL8wjwn0Jwr"
   },
   "outputs": [],
   "source": [
    "# test the model with some data\n",
    "\n",
    "optimizer_name = \"SGD\"\n",
    "learning_rate = 0.01\n",
    "seq_length = 9\n",
    "batch_size = len(train_dev_data)\n",
    "\n",
    "num_epochs = 50\n",
    "input_size = 1 # \"channels\" of data\n",
    "num_layers = 2 # depth of model (number of \"stacks\" of hidden layers)\n",
    "hidden_size = 16 # breadth of model (number of units in hidden layers)\n",
    "seq_length  = 9 # number of datapoints used for learning in each segment\n",
    "\n",
    "# test the model with one batch\n",
    "net,lossfun,optimizer = createModel(optimizer_name, learning_rate, input_size, num_layers, hidden_size)\n",
    "\n",
    "\n",
    "X = (train_dev_data[:, :seq_length]).view(batch_size,seq_length,1)\n",
    "\n",
    "print(X.shape)\n",
    "y_pred = net(X)\n",
    "\n",
    "\n",
    "print(y_pred.shape)\n",
    "\n",
    "# print(y)\n",
    "# grab the final predicted value from the output (first element of tuple output of net)\n",
    "finalValue = y_pred[0][-1]\n",
    "print(finalValue)\n",
    "\n",
    "lossfun = nn.MSELoss()\n",
    "lossfun(finalValue,train_dev_data[:, seq_length].view(batch_size,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUMPfhoFWqrA"
   },
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aUKZKn5Wqt-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "debug = False\n",
    "\n",
    "import sys\n",
    "\n",
    "def custom_balanced_accuracy_score(pred_value, true_value, threshold=103):\n",
    "    # Convert regression values to binary classes\n",
    "    pred_classes = (pred_value > threshold).astype(int)\n",
    "    true_classes = (true_value > threshold).astype(int)\n",
    "    return balanced_accuracy_score(true_classes, pred_classes) * 100\n",
    "\n",
    "def trainModel(net, lossfun, optimizer, num_epochs, train_data, dev_data, seq_length, num_layers, hidden_size):\n",
    "    N_train = len(train_data[1])\n",
    "    N_dev = len(dev_data[1])\n",
    "\n",
    "    train_batch_size  = len(train_data[:, 0]) \n",
    "    dev_batch_size  = len(dev_data[:, 0]) \n",
    "\n",
    "    device = next(net.parameters()).device  # Use model's device\n",
    "\n",
    "    # Track metrics\n",
    "    losses = np.zeros(num_epochs)\n",
    "    accuracyA = np.zeros(num_epochs)\n",
    "    printed = False\n",
    "\n",
    "    # Start measuring time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize trackers\n",
    "    trainLosses = torch.zeros(num_epochs)\n",
    "    devLosses  = torch.zeros(num_epochs)\n",
    "    trainAcc    = []\n",
    "    devAcc     = []\n",
    "    all_metrics = []  # To store (precision, recall, f1, balanced_acc) per fold FOR TRAIN\n",
    "    dev_all_metrics = []  # To store (precision, recall, f1, balanced_acc) per fold FOR DEV\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        \n",
    "        segment_losses = []\n",
    "        segment_acc = []\n",
    "        segment_all_metrics = []\n",
    "        \n",
    "        # Reset hidden state (for stateless RNNs)\n",
    "        hidden_state = torch.zeros(num_layers, train_batch_size, hidden_size).to(device)\n",
    "\n",
    "        # Iterate over the test sequence data to create sliding windows of length seq_length for input and predict the next value.\n",
    "        for timei in range(N_train - seq_length):\n",
    "            # Extract the training sequence\n",
    "            X = train_data[:, timei:timei+seq_length].view(train_batch_size, seq_length, 1).to(device)\n",
    "            # Extract the label or the last item of the sequence which indicates if link is broken or not\n",
    "            y = train_data[:, timei + seq_length].view(train_batch_size, 1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = net(X)\n",
    "            loss = lossfun(y_pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save the losses for each sequence of length : seq_length (In my case each line has only  one sequence of length seq_length + 1 )\n",
    "            segment_losses.append(loss.item())\n",
    "            true_value = y.cpu().numpy().squeeze()\n",
    "            pred_value = y_pred.detach().cpu().numpy().squeeze()\n",
    "         \n",
    "            accuracy = custom_balanced_accuracy_score(pred_value, true_value)\n",
    "            segment_acc.append(accuracy)\n",
    "\n",
    "\n",
    "        # Save the losses and accuracy for for each epoch \n",
    "        trainLosses[epoch] = torch.tensor(segment_losses).mean()\n",
    "        trainAcc.append(np.mean(segment_acc))\n",
    "\n",
    "        # --- Evaluation on dev dataset ---\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            segment_losses = []\n",
    "            segment_acc = []\n",
    "            segment_all_metrics = []\n",
    "\n",
    "            for timei in range(N_dev - seq_length):\n",
    "                X_dev = dev_data[:, timei:timei+seq_length].view(dev_batch_size, seq_length, 1).to(device)\n",
    "                y_dev = dev_data[:, timei + seq_length].view(dev_batch_size, 1).to(device)\n",
    "\n",
    "                y_dev_pred = net(X_dev)\n",
    "                dev_loss = lossfun(y_dev_pred, y_dev)\n",
    "\n",
    "                segment_losses.append(dev_loss.item())\n",
    "\n",
    "            \n",
    "                true_value = y_dev.cpu().numpy().squeeze()\n",
    "                pred_value = y_dev_pred.detach().cpu().numpy().squeeze()\n",
    "\n",
    "                accuracy = custom_balanced_accuracy_score(pred_value, true_value)\n",
    "                segment_acc.append(accuracy)\n",
    "\n",
    "\n",
    "            devLosses[epoch] = torch.tensor(segment_losses).mean()\n",
    "            devAcc.append(np.mean(segment_acc))\n",
    "\n",
    "            if debug:\n",
    "                sys.stdout.write(\n",
    "                    f'\\rEpoch {epoch+1}/{num_epochs} - '\n",
    "                    f'Train Loss: {trainLosses[epoch]:.4f} - '\n",
    "                    f'Dev Loss: {devLosses[epoch]:.4f} - '\n",
    "                    f'Balanced Train Acc: {trainAcc[epoch]:.2f}% - '\n",
    "                    f'Balanced Dev Acc: {devAcc[epoch]:.2f}%'\n",
    "                )\n",
    "                sys.stdout.flush()\n",
    "\n",
    "           \n",
    "\n",
    "    # Total time\n",
    "    total_time = time.time() - start_time\n",
    "    if debug:\n",
    "        print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "\n",
    "    return net, trainAcc, devAcc, trainLosses, devLosses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doIt():       \n",
    "    k_folds = 5\n",
    "    num_repeats = 3  # P times : K-Fold multiple times (P iterations) \n",
    "    \n",
    "    # === STORAGE FOR METRICS\n",
    "    final_train_acc = []\n",
    "    final_dev_acc = []\n",
    "    final_train_losses = []\n",
    "    final_dev_losses = []\n",
    "    \n",
    "    mean_train_acc = []\n",
    "    mean_dev_acc = []\n",
    "    mean_train_losses = []\n",
    "    mean_dev_losses = []\n",
    "    \n",
    "    # Convert to numpy\n",
    "    X_np = train_dev_data.cpu().numpy() if isinstance(train_dev_data, torch.Tensor) else train_dev_data\n",
    "    # Create labels based on the last column values (>103 or <=103)\n",
    "    labels = np.where(X_np[:, -1] > 103, 1, 0)  # 1 if >103, otherwise 0\n",
    "    \n",
    "\n",
    "    for repeat in range(num_repeats):\n",
    "        if debug:\n",
    "            print(f\"\\n{Fore.RED}{Style.BRIGHT}=== Iteration {repeat+1}/{num_repeats} ==={Style.RESET_ALL}\")\n",
    "        \n",
    "        all_fold_train_acc = []\n",
    "        all_fold_dev_acc = []\n",
    "        \n",
    "        all_fold_train_losses = []\n",
    "        all_fold_dev_losses = []\n",
    "    \n",
    "        skf = StratifiedKFold(n_splits=k_folds, shuffle=True) \n",
    "        \n",
    "        for fold, (train_idx, dev_idx) in enumerate(skf.split(X_np, labels)):\n",
    "            if debug:\n",
    "                print(f\"\\n{Fore.GREEN}{Style.BRIGHT}=== Fold {fold+1}/{k_folds} ==={Style.RESET_ALL}\")\n",
    "        \n",
    "            # Create train/dev datasets\n",
    "            X_train, X_dev = train_dev_data[train_idx], train_dev_data[dev_idx]\n",
    "  \n",
    "            # === MODEL CREATION ===\n",
    "            RNN, lossfun, optimizer = createModel(optimizer_name, learning_rate, input_size, num_layers, hidden_size)\n",
    "            RNN.to(device)\n",
    "        \n",
    "            # Train\n",
    "            net, trainAcc, devAcc, trainLosses, devLosses = trainModel(\n",
    "                RNN, lossfun, \n",
    "                optimizer, \n",
    "                num_epochs, \n",
    "                X_train, \n",
    "                X_dev, \n",
    "                seq_length, \n",
    "                num_layers, \n",
    "                hidden_size\n",
    "            )\n",
    "    \n",
    "            final_train_acc = trainAcc[-1]\n",
    "            final_dev_acc = devAcc[-1]\n",
    "        \n",
    "            all_fold_train_losses.append(trainLosses)\n",
    "            all_fold_dev_losses.append(devLosses)\n",
    "            \n",
    "            all_fold_train_acc.append(trainAcc)\n",
    "            all_fold_dev_acc.append(devAcc)\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Fold {fold+1} Final Train Accuracy: {final_train_acc:.2f}%\")\n",
    "                print(f\"Fold {fold+1} Final DEV Accuracy: {final_dev_acc:.2f}%\")\n",
    "    \n",
    "        # calculate mean for this iteration\n",
    "        mean_train_losses.append(np.mean(all_fold_train_losses, axis=0))\n",
    "        mean_dev_losses.append(np.mean(all_fold_dev_losses, axis=0))\n",
    "        \n",
    "        mean_train_acc.append(np.mean(all_fold_train_acc, axis=0))\n",
    "        mean_dev_acc.append(np.mean(all_fold_dev_acc, axis=0))\n",
    "\n",
    "    \n",
    "    return net, lossfun, mean_train_losses, mean_dev_losses, mean_train_acc, mean_dev_acc\n",
    "    # incrementalTrainRNN(net, optimizer, data, newdata, seq_length, batch_size, num_layers, hidden_size)\n",
    "\n",
    "\n",
    "net, lossfun, mean_train_losses, mean_dev_losses, mean_train_acc, mean_dev_acc = doIt()\n",
    "\n",
    "\n",
    "# calculate mean for all iterations\n",
    "final_train_losses = np.mean(mean_train_losses, axis=0)\n",
    "final_dev_losses = np.mean(mean_dev_losses, axis=0)\n",
    "final_train_acc = np.mean(mean_train_acc, axis=0)\n",
    "final_dev_acc = np.mean(mean_dev_acc, axis=0)\n",
    "\n",
    "print(\"################################################\")\n",
    "print(f\"Final Train Losses: {final_train_losses[-1]:.2f}\")\n",
    "print(f\"Final Dev Losses: {final_dev_losses[-1]:.2f}\")\n",
    "print(f\"Final Balanced Train Accuracy: {final_train_acc[-1]:.2f}%\")\n",
    "print(f\"Final Balanced Dev Accuracy: {final_dev_acc[-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "optimizer_name = \"Adam\"\n",
    "learning_rate = 0.01\n",
    "\n",
    "use_batchnorm = True\n",
    "use_dropout = True\n",
    "\n",
    "num_epochs = 3000\n",
    "input_size = 1 # \"channels\" of data\n",
    "num_layers = 3 # depth of model (number of \"stacks\" of hidden layers)\n",
    "hidden_size = 32 # breadth of model (number of units in hidden layers)\n",
    "seq_length  = 9 # number of datapoints used for learning in each segment\n",
    "# batch_size  = len(train_data[:, 0]) # Note: the training code is actually hard-coded to organize data into batch_size=1\n",
    "\n",
    "\n",
    "# These for Hyperparameter Tuning\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "num_layers_list = [2, 3, 4]\n",
    "dropout_rates = [False, True]\n",
    "\n",
    "# These are the best selected Hyperparameter (after exprementation)\n",
    "learning_rates = [0.01]\n",
    "num_layers_list = [3]\n",
    "dropout_rates = [True]\n",
    "\n",
    "num = 0\n",
    "for dropout in dropout_rates:\n",
    "    use_dropout = dropout\n",
    "    for lr in learning_rates:\n",
    "        learning_rate = lr\n",
    "        for num_l in num_layers_list:\n",
    "            num_layers = num_l\n",
    "            num +=1\n",
    "            print(f\"#{num}   ################################################\")\n",
    "            print(f\"Learning Rate: {lr}, Number of Layers: {num_l}, Dropout Rate: {dropout}\")\n",
    "            # Here you can call your training function with these parameters\n",
    "            net, lossfun, mean_train_losses, mean_dev_losses, mean_train_acc, mean_dev_acc = doIt()\n",
    "            # calculate mean for all iterations\n",
    "            final_train_losses = np.mean(mean_train_losses, axis=0)\n",
    "            final_dev_losses = np.mean(mean_dev_losses, axis=0)\n",
    "            final_train_acc = np.mean(mean_train_acc, axis=0)\n",
    "            final_dev_acc = np.mean(mean_dev_acc, axis=0)\n",
    "            \n",
    "            print(f\"Final Balanced Dev Accuracy: {final_dev_acc[-1]:.2f}%\")\n",
    "            print(\"################################################\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_Au7fIUWq0H"
   },
   "outputs": [],
   "source": [
    "print(f\"Train accuracy : {final_train_acc[-1]:.2f}%\")\n",
    "print(f\"Dev accuracy : {final_dev_acc[-1]:.2f}%\")\n",
    "\n",
    "\n",
    "net.eval()\n",
    "\n",
    "\n",
    "# plot the results\n",
    "fig,axs = plt.subplots(1, 2,figsize=(17,5))\n",
    "\n",
    "axs[0].plot(final_train_losses, label='Train losses')\n",
    "axs[0].plot(final_dev_losses, label='Dev losses')\n",
    "axs[0].set_title('Train/Dev Losses over k-folds')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(final_train_acc,label='Train')\n",
    "axs[1].legend()\n",
    "# plt.xlim(0, 5)\n",
    "axs[1].plot(final_dev_acc,label='Dev')\n",
    "axs[1].set_title(f'Train & Dev balanced accuracy over k-folds = {final_dev_acc[-1]:.2f}%')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation on test dataset ---\n",
    "N_test = len(test_data[1])\n",
    "test_batch_size  = len(test_data[:, 0])\n",
    "\n",
    "\n",
    "seq_length  = 9\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    segment_losses = []\n",
    "    segment_acc = []\n",
    "    \n",
    "    for timei in range(N_test - seq_length):\n",
    "        X_test = test_data[:, timei:timei+seq_length].view(test_batch_size, seq_length, 1).to(device)\n",
    "        y_test = test_data[:, timei + seq_length].view(test_batch_size, 1).to(device)\n",
    "\n",
    "        y_test_pred = net(X_test)\n",
    "        test_loss = lossfun(y_test_pred, y_test)\n",
    "\n",
    "        segment_losses.append(test_loss.item())\n",
    "\n",
    "        true_value = y_test.cpu().numpy().squeeze()\n",
    "        pred_value = y_test_pred.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        acc = custom_balanced_accuracy_score(pred_value, true_value)\n",
    "        segment_acc.append(acc)\n",
    "\n",
    "    losses = torch.tensor(segment_losses).mean()\n",
    "    accuracy = np.mean(segment_acc)\n",
    "\n",
    "\n",
    "sys.stdout.write(f'\\r Test Loss: {losses:.4f} - Test Acc: {accuracy:.2f}%')\n",
    "sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPD7DQOJ9re8ygNIq6i8lv",
   "collapsed_sections": [],
   "name": "DUDL_RNN_altSequences.ipynb",
   "provenance": [
    {
     "file_id": "19WUFNKOHKnZ1hEkR6havrl_eIE4BxuaE",
     "timestamp": 1621268036592
    },
    {
     "file_id": "1BI9p-vnVoi7Tm8yiQgVN3kpM2VuEzfeE",
     "timestamp": 1621245811372
    },
    {
     "file_id": "1o_dLKV6fY7xdZYx_pNMY12zpL_pmMurs",
     "timestamp": 1618865813618
    },
    {
     "file_id": "1Q9LtmanyNt675-gO_kXRBKalCdP6xtvV",
     "timestamp": 1617253457100
    },
    {
     "file_id": "1jeqKEJfI18GlAhSG8RO5aJ6Vrp4-nkTt",
     "timestamp": 1615909315432
    },
    {
     "file_id": "10_geQnah5AvMsm8VDAQwNPhypOXradar",
     "timestamp": 1615893340208
    },
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615877547147
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
